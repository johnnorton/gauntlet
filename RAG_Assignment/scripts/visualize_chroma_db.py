"""
VISUALIZE CHROMA DATABASE
===========================
Shows what data looks like in the vector database.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.index import get_collection
import numpy as np

print("\n" + "â–ˆ"*80)
print("â–ˆ" + " "*78 + "â–ˆ")
print("â–ˆ" + " "*20 + "WHAT'S IN THE CHROMA VECTOR DATABASE?" + " "*24 + "â–ˆ")
print("â–ˆ" + " "*78 + "â–ˆ")
print("â–ˆ"*80)

# Get the Chroma collection
collection = get_collection()

print(f"\n1ï¸âƒ£  DATABASE STATS")
print("â”€" * 80)

count = collection.count()
print(f"âœ… Total chunks stored: {count}")
print(f"   Vector dimensions: 384 (sentence-transformers/all-MiniLM-L6-v2)")
print(f"   Database location: data/chroma_db/")
print(f"   Index type: HNSW (hierarchical navigable small world)")

print(f"\n\n2ï¸âƒ£  DATA STRUCTURE - WHAT ONE CHUNK LOOKS LIKE")
print("â”€" * 80)

# Get first chunk to show structure
results = collection.get(limit=1)

if results and results['documents']:
    chunk_text = results['documents'][0]
    metadata = results['metadatas'][0]
    embedding = results['embeddings'][0] if results['embeddings'] else None
    chunk_id = results['ids'][0]

    print(f"\nğŸ“„ CHUNK ID: {chunk_id}\n")

    print(f"ğŸ“ TEXT CONTENT (what gets retrieved):")
    print(f"{'â”€'*80}")
    # Show first 500 chars of the text
    print(chunk_text[:500])
    if len(chunk_text) > 500:
        print(f"... ({len(chunk_text)} total characters)")
    print(f"{'â”€'*80}\n")

    print(f"ğŸ·ï¸  METADATA (searchable fields):")
    print(f"{'â”€'*80}")
    for key, value in sorted(metadata.items()):
        print(f"   {key:.<30} {str(value)[:50]}")
    print(f"{'â”€'*80}\n")

    if embedding:
        print(f"ğŸ”¢ EMBEDDING VECTOR (384 dimensions):")
        print(f"{'â”€'*80}")
        print(f"   First 10 dimensions:    {[round(x, 4) for x in embedding[:10]]}")
        print(f"   Middle 10 dimensions:   {[round(x, 4) for x in embedding[187:197]]}")
        print(f"   Last 10 dimensions:     {[round(x, 4) for x in embedding[-10:]]}")

        # Calculate vector stats
        vec_array = np.array(embedding)
        print(f"")
        print(f"   Vector magnitude:       {np.linalg.norm(vec_array):.4f}")
        print(f"   Mean value:             {np.mean(vec_array):.4f}")
        print(f"   Min value:              {np.min(vec_array):.4f}")
        print(f"   Max value:              {np.max(vec_array):.4f}")
        print(f"{'â”€'*80}\n")

print(f"\n3ï¸âƒ£  EXAMPLE - HOW CHROMA STORES DATA")
print("â”€" * 80)

print(f"""
Chroma stores each chunk like this:

{{"
  "id": "chunk_001_block_1",
  "document": "Invoice: INV001\\nDate: 1/15/2024\\nComplaint: Engine won't start...",
  "embedding": [0.0234, -0.0156, 0.0892, ...384 numbers total...],
  "metadata": {{
    "invoice_id": "INV001",
    "customer_name": "John's Garage",
    "vehicle_year": "2020",
    "vehicle_make": "Ford",
    "vehicle_model": "F-150",
    "service_block_num": "1",
    "complaint": "Engine won't start",
    "cause": "Dead battery",
    "correction": "Replaced battery",
    "labor_hours": "0.5",
    "labor_rate": "100.0"
  }}
}}

IMPORTANT: The embedding is what makes semantic search work!
- When you ask a question, it gets embedded to 384 dimensions
- Chroma compares your question embedding to all 1,564 chunk embeddings
- Returns chunks with HIGHEST similarity scores (0-1 scale)
- Typically returns top 5-10 most similar chunks
""")

print(f"\n4ï¸âƒ£  HOW RETRIEVAL WORKS")
print("â”€" * 80)

print(f"""
Query Flow:
1. User asks: "What electrical problems were found?"
2. Query embedding: "What electrical problems were found?" â†’ [0.045, -0.032, ...]
3. Similarity search: Compare query embedding to all 1,564 chunk embeddings
4. Score each chunk: cosine_similarity(query_embedding, chunk_embedding)
5. Sort by score (highest = most similar)
6. Return top K (usually K=5)

Similarity scores are between 0 and 1:
  1.0 = Perfect match (identical embeddings)
  0.8+ = Highly relevant
  0.6-0.7 = Somewhat relevant
  <0.5 = Not very relevant
""")

print(f"\n5ï¸âƒ£  EXAMPLE RETRIEVAL - SEARCH IN ACTION")
print("â”€" * 80)

# Do a real search
from src.embed import embed_single_chunk

query = "What electrical problems were found?"
query_embedding = embed_single_chunk(query)

print(f"\nQuery: \"{query}\"")
print(f"Query embedding (first 10 dimensions): {[round(x, 4) for x in query_embedding[:10]]}\n")

# Search Chroma
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=3,
    include=["documents", "metadatas", "distances"]
)

print(f"âœ… Found {len(results['documents'][0])} results:\n")

for i, (doc, meta, distance) in enumerate(zip(
    results['documents'][0],
    results['metadatas'][0],
    results['distances'][0]
), 1):
    # Convert distance to similarity (Chroma returns distance, not similarity)
    similarity = 1 - (distance / 2)  # Approximate similarity

    complaint = meta.get('complaint') or "N/A"
    if complaint != "N/A":
        complaint = complaint[:60] + "..."

    print(f"Result {i}:")
    print(f"  Similarity score: {similarity:.4f} (0-1 scale, higher = better)")
    print(f"  Invoice: {meta.get('invoice_id')}")
    print(f"  Customer: {meta.get('customer_name')}")
    print(f"  Complaint: {complaint}")
    print(f"  Text preview: {doc[:70]}...")
    print()

print(f"\n6ï¸âƒ£  DATABASE ARCHITECTURE")
print("â”€" * 80)

print(f"""
Chroma Database Structure:

data/chroma_db/
â”œâ”€â”€ index.bin          # HNSW index (for fast similarity search)
â”œâ”€â”€ metadata.db        # SQLite database (stores metadata)
â”œâ”€â”€ data.db            # Embeddings and documents
â””â”€â”€ tenant-data.db     # Collection info

Why HNSW index?
  â€¢ Hierarchical Navigable Small World algorithm
  â€¢ Fast approximate nearest neighbor search
  â€¢ <1ms query time for 1,564 vectors
  â€¢ Much faster than brute-force search (which would be O(n))

Vector Storage:
  â€¢ Format: 384-dimensional float32 vectors
  â€¢ Size per vector: 384 Ã— 4 bytes = 1,536 bytes
  â€¢ Total storage: 1,564 Ã— 1,536 bytes â‰ˆ 2.4 MB
  â€¢ Metadata: Additional ~5-10 MB (depends on text length)
  â€¢ Total database size: ~10-15 MB (very lightweight!)
""")

print(f"\n7ï¸âƒ£  METADATA FIELDS (What Gets Indexed)")
print("â”€" * 80)

# Get all unique metadata fields
all_data = collection.get()
if all_data['metadatas']:
    all_keys = set()
    for metadata in all_data['metadatas']:
        all_keys.update(metadata.keys())

    print(f"\nSearchable metadata fields:\n")
    for i, key in enumerate(sorted(all_keys), 1):
        print(f"  {i:2d}. {key}")

print(f"\n\n8ï¸âƒ£  WHY THIS MATTERS FOR RAG")
print("â”€" * 80)

print(f"""
The vector database is the CORE of your RAG system:

Query â†’ Embed â†’ ğŸ” SEARCH IN DATABASE â†’ Retrieve Top-K â†’ Generate Answer
                  â†‘
                  You are here!

Without good vectors and storage:
  âŒ Can't find relevant chunks
  âŒ Get irrelevant context
  âŒ Claude generates bad answers

With Chroma database:
  âœ… Fast semantic search (<1ms per query)
  âœ… Stores 1,564 chunks compactly (~15 MB)
  âœ… Easy retrieval with similarity scoring
  âœ… Perfect for production RAG systems

Your setup:
  â€¢ 1,564 service block chunks
  â€¢ 384-dimensional embeddings
  â€¢ HNSW indexing for fast search
  â€¢ Local persistent storage (no API calls needed)
  â€¢ ~15 MB total database size
""")

print(f"\n{'â•'*80}\n")

print(f"ğŸ’¡ TRY THIS:")
print(f"""
1. Open data/chroma_db/ to see the actual database files
2. Run: python scripts/query.py "electrical problem"
3. See what gets retrieved and why!
""")

print(f"\n{'â•'*80}\n")
