"""
INTERACTIVE RAG PIPELINE DEMO WITH SAMPLE QUESTIONS
====================================================
Step through the RAG process, then answer 5 real questions about the data.
Perfect for recording - shows use cases at start, demonstrates them at end.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import os
import sys
from io import StringIO
from contextlib import contextmanager

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Suppress logging
import logging
logging.disable(logging.CRITICAL)

from src.pipeline import run_rag_pipeline
from src.embed import get_embedding_model
import time
import threading

@contextmanager
def suppress_stderr():
    """Context manager to suppress stderr output from libraries"""
    save_stderr = sys.stderr
    sys.stderr = StringIO()
    try:
        yield
    finally:
        sys.stderr = save_stderr

def animated_processing(duration_callback=None):
    """
    Show animated processing indicator with timer.
    Returns a function to stop the animation.
    """
    spinner_frames = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']
    stop_animation = threading.Event()
    start_time = time.time()

    def animate():
        frame = 0
        while not stop_animation.is_set():
            elapsed = time.time() - start_time
            sys.stdout.write(f'\r{spinner_frames[frame % len(spinner_frames)]} Processing... ({elapsed:.1f}s)')
            sys.stdout.flush()
            time.sleep(0.1)
            frame += 1
        # Clear the line
        sys.stdout.write('\r' + ' ' * 40 + '\r')
        sys.stdout.flush()

    thread = threading.Thread(target=animate, daemon=True)
    thread.start()

    def stop():
        stop_animation.set()
        thread.join(timeout=1)

    return stop

def pause(message="Press ENTER to continue..."):
    """Pause execution and wait for user input"""
    print(f"\n{'â”€'*80}")
    input(f"\nâ¸ï¸  {message}\n")
    print(f"{'â”€'*80}\n")

def section(title):
    """Print a section header"""
    print(f"\n{'â•'*80}")
    print(f"  {title}")
    print(f"{'â•'*80}\n")

# ============================================================================
# DEMO STARTS HERE
# ============================================================================

print("\n" + "â–ˆ"*80)
print("â–ˆ" + " "*78 + "â–ˆ")
print("â–ˆ" + " "*10 + "COMPLETE RAG PIPELINE DEMONSTRATION" + " "*33 + "â–ˆ")
print("â–ˆ" + " "*10 + "With Real-World Questions & Answers" + " "*30 + "â–ˆ")
print("â–ˆ" + " "*78 + "â–ˆ")
print("â–ˆ"*80)

# OPENING: Show what questions we'll answer
section("INTRODUCTION: SAMPLE QUESTIONS")

questions_to_ask = [
    "What electrical problems were found on Fords?",
    "What are the most common service issues across all vehicles?",
    "Which vehicles required battery work and what was done?",
    "What brake-related repairs were performed?",
    "What fixes were applied to charging system issues?"
]

print("""
This RAG system can answer real questions about our truck service data.
Let's start by identifying 5 questions we want to answer:

""")

for i, q in enumerate(questions_to_ask, 1):
    print(f"  {i}. {q}")

print(f"""

These questions represent different types of queries:
  â€¢ Vehicle-specific (What about Fords?)
  â€¢ Problem-specific (Electrical issues? Charging? Brakes?)
  â€¢ Pattern-finding (Most common issues?)
  â€¢ General repairs (What was done for X?)

Let's walk through HOW the system answers these questions.
""")

pause("Ready to start the demo? Press ENTER...")

# STEP 1: INGESTION
section("STEP 1: DATA INGESTION (Already Completed)")

print("""
From our previous run, we extracted and processed:

ðŸ“Š EXTRACTION PHASE
   â”œâ”€ Source: 1,000 PDF invoices
   â”œâ”€ Parsed with regex patterns
   â”œâ”€ Success rate: 97.2% (972 parsed correctly)
   â””â”€ Result: Structured invoice data

ðŸ“¦ CHUNKING PHASE
   â”œâ”€ Strategy: Service-block level
   â”œâ”€ 1 chunk = 1 service repair + invoice context
   â”œâ”€ Total chunks: 1,564
   â””â”€ From 813 unique invoices

âš¡ EMBEDDING PHASE
   â”œâ”€ Model: sentence-transformers/all-MiniLM-L6-v2
   â”œâ”€ Each chunk â†’ 384-dimensional vector
   â”œâ”€ 1,564 embeddings created (~2-3 seconds)
   â””â”€ No API rate limits (local model)

ðŸ—„ï¸  INDEXING PHASE
   â”œâ”€ Database: Chroma (with HNSW indexing)
   â”œâ”€ Storage: data/chroma_db/
   â”œâ”€ Total index size: ~15 MB
   â””â”€ Ready for queries!
""")

pause("Data is ready. Press ENTER to continue to the technical walkthrough...")

# STEP 2: TECHNICAL WALKTHROUGH
section("STEP 2-8: TECHNICAL WALKTHROUGH (Quick Version)")

print("""
We're going to walk through the complete RAG pipeline quickly, then 
answer all 5 sample questions with real data.

The pipeline works like this:

  QUERY (text)
    â†“
  EMBED (convert to 384-dim vector)
    â†“
  SEARCH (compare to 1,564 chunk vectors)
    â†“
  RETRIEVE (get top-50 most similar chunks)
    â†“
  BUILD PROMPT (system + question + 50 chunks)
    â†“
  SEND TO CLAUDE (via API)
    â†“
  CLAUDE SYNTHESIZES (reads 50 chunks, makes answer)
    â†“
  RETURN ANSWER (with sources cited)
""")

pause("Ready to see it in action? Press ENTER to answer the 5 questions...")

# PART 2: ANSWER THE QUESTIONS
section("ANSWERING THE SAMPLE QUESTIONS")

print(f"""
Now let's answer all {len(questions_to_ask)} questions we identified at the start.

For each question:
  1. We'll show the question
  2. The system will retrieve and search
  3. Claude will generate an answer
  4. We'll show sources cited

Let's begin...
""")

pause("Ready? Press ENTER...")

# Preload embedding model to avoid timeout on first question
print("â³ Loading embedding model... (this happens once)\n")
with suppress_stderr():
    get_embedding_model()
print("âœ… Model loaded!\n")

# Answer each question
answers = []
for i, question in enumerate(questions_to_ask, 1):
    section(f"QUESTION {i}/{len(questions_to_ask)}")

    print(f"â“ Question: \"{question}\"\n")

    # Show what's about to happen
    print("This query will:")
    print("  â€¢ Embed the question (convert to 384-dimensional vector)")
    print("  â€¢ Search 1,564 chunks for the most similar ones")
    print("  â€¢ Retrieve top-50 chunks from the database")
    print("  â€¢ Send them to Claude along with your question")
    print("  â€¢ Claude will synthesize an intelligent answer")
    print("  â€¢ We'll show you which invoices were cited as sources\n")

    pause(f"Ready to process Question {i}? Press ENTER to start...")

    print(f"â“ \"{question}\"\n")

    # Start animated processing
    stop_animation = animated_processing()

    start = time.time()
    with suppress_stderr():
        result = run_rag_pipeline(question, k=50)
    elapsed = time.time() - start

    # Stop animation
    stop_animation()
    print(f"âœ… Complete! ({elapsed:.2f} seconds)\n")

    print(f"ðŸ“Š STATISTICS:")
    print(f"   â€¢ Processing time: {elapsed:.2f} seconds")
    print(f"   â€¢ Chunks retrieved: {len(result['retrieved_chunks'])}")
    print(f"   â€¢ Invoices cited: {len(result['source_invoices'])}\n")

    # Show what was sent to Claude
    print(f"{'â”€'*80}")
    print(f"ðŸ“¬ WHAT WAS SENT TO CLAUDE")
    print(f"{'â”€'*80}\n")

    system_prompt = """You are a helpful assistant that answers questions about truck service invoices.
Answer questions based ONLY on the provided invoice context. If the answer is not in the context,
say "I cannot find this information in the provided invoices." Be specific and cite the invoices when relevant."""

    context_parts = [chunk["text"] for chunk in result['retrieved_chunks']]
    context = "\n\n---\n\n".join(context_parts)

    user_prompt = f"""Based on the following invoice context, answer this question: {question}

INVOICE CONTEXT:
{context}

Please provide a clear, concise answer based only on the information above."""

    print(f"ðŸ”· SYSTEM PROMPT ({len(system_prompt)} chars):")
    print(f"   \"{system_prompt}\"\n")

    print(f"ðŸ”· USER PROMPT STRUCTURE ({len(user_prompt):,} chars total):")
    print(f"   â”œâ”€ Question: {len(question)} chars")
    print(f"   â”œâ”€ Context: {len(context):,} chars (50 chunks)")
    print(f"   â””â”€ Instructions: ~80 chars\n")

    print(f"ðŸ”· USER PROMPT START (first 500 chars):")
    print(f"   {user_prompt[:500]}...\n")

    total_chars = len(system_prompt) + len(user_prompt)
    input_tokens = total_chars // 4

    # Estimate output tokens from Claude's response
    response_text = result['answer']
    output_tokens = len(response_text.split()) * 1.3  # Rough estimate: 1 word â‰ˆ 1.3 tokens

    # Claude 3.5 Sonnet pricing (as of Jan 2025)
    input_cost_per_mtok = 3.00      # $3 per 1M input tokens
    output_cost_per_mtok = 15.00    # $15 per 1M output tokens

    # Calculate costs
    input_cost = (input_tokens / 1_000_000) * input_cost_per_mtok
    output_cost = (output_tokens / 1_000_000) * output_cost_per_mtok
    total_cost = input_cost + output_cost

    print(f"ðŸ“ˆ TOKEN USAGE:")
    print(f"   â€¢ Input tokens: {int(input_tokens):,}")
    print(f"   â€¢ Output tokens: {int(output_tokens):,}")
    print(f"   â€¢ Total tokens: {int(input_tokens + output_tokens):,}")
    print(f"   â€¢ Claude's limit: 200,000 tokens")
    print(f"   â€¢ Usage: {((input_tokens + output_tokens) / 200000 * 100):.1f}% (very efficient!)\n")

    print(f"ðŸ’° COST ANALYSIS (Claude 3.5 Sonnet):")
    print(f"   â€¢ Input cost: ${input_cost:.6f} ({int(input_tokens):,} tokens Ã— $3/1M)")
    print(f"   â€¢ Output cost: ${output_cost:.6f} ({int(output_tokens):,} tokens Ã— $15/1M)")
    print(f"   â€¢ TOTAL PER QUESTION: ${total_cost:.6f}\n")

    print(f"{'â”€'*80}\n")

    print(f"ðŸ“ CLAUDE'S ANSWER:\n")
    print(result['answer'])
    
    print(f"\nðŸ“Œ SOURCES CITED:")
    for inv_id in sorted(result['source_invoices'])[:8]:
        print(f"   â€¢ {inv_id}")
    if len(result['source_invoices']) > 8:
        print(f"   â€¢ ... and {len(result['source_invoices']) - 8} more invoices")

    answers.append({
        'question': question,
        'answer': result['answer'],
        'sources': result['source_invoices'],
        'time': elapsed,
        'input_tokens': int(input_tokens),
        'output_tokens': int(output_tokens),
        'cost': total_cost
    })

    if i < len(questions_to_ask):
        pause(f"Great! Ready for Question {i+1}? Press ENTER to continue...")

# SUMMARY
section("SUMMARY: ALL QUESTIONS ANSWERED")

print(f"""
We successfully answered all {len(questions_to_ask)} sample questions!

Summary:
""")

for i, item in enumerate(answers, 1):
    print(f"\n{i}. Question: \"{item['question']}\"")
    print(f"   âœ“ Time: {item['time']:.2f}s | Tokens: {item['input_tokens'] + item['output_tokens']:,} | Cost: ${item['cost']:.6f}")
    print(f"   âœ“ Sources: {len(item['sources'])} invoices cited")
    # Show first 100 chars of answer
    preview = item['answer'].split('\n')[0][:70]
    print(f"   âœ“ Answer preview: {preview}...")

total_tokens = sum(a['input_tokens'] + a['output_tokens'] for a in answers)
total_cost = sum(a['cost'] for a in answers)
avg_cost_per_question = total_cost / len(answers)

print(f"""

PERFORMANCE METRICS:
  â€¢ Total questions answered: {len(answers)}
  â€¢ Total processing time: {sum(a['time'] for a in answers):.2f} seconds
  â€¢ Average per question: {sum(a['time'] for a in answers) / len(answers):.2f} seconds
  â€¢ Unique invoices cited: {len(set(inv for a in answers for inv in a['sources']))}

COST ANALYSIS:
  â€¢ Total tokens used: {total_tokens:,}
  â€¢ Total cost: ${total_cost:.6f} (all 5 questions)
  â€¢ Average cost per question: ${avg_cost_per_question:.6f}
  â€¢ Cost per 1,000 queries: ${avg_cost_per_question * 1000:.2f}

  Pricing: Claude 3.5 Sonnet - $3/1M input tokens, $15/1M output tokens

This demonstrates:
  âœ“ Semantic search quickly finding relevant data
  âœ“ LLM synthesizing coherent, intelligent answers
  âœ“ Source citation for full traceability
  âœ“ Real-time performance (seconds, not minutes)
  âœ“ Handling diverse query types (vehicle-specific, problem-specific, pattern-finding)
  âœ“ Efficient token usage (only 7.4% of available tokens)
  âœ“ Cost-effective AI usage (fractions of a cent per question!)
""")

pause("Ready for final summary? Press ENTER...")

# FINAL SUMMARY
section("RAG PIPELINE COMPLETE")

print(f"""
What we demonstrated:

1. INGESTION PHASE
   â€¢ 1,000 PDFs â†’ 1,564 searchable chunks
   â€¢ Service-block level granularity
   â€¢ 384-dimensional embeddings

2. RETRIEVAL PHASE (Semantic Search)
   â€¢ Query embedding
   â€¢ Compare to all 1,564 vectors
   â€¢ Get 50 most similar chunks
   â€¢ Filters from 1,564 â†’ 50

3. GENERATION PHASE (Claude LLM)
   â€¢ Claude reads 50 chunks
   â€¢ Understands relationships
   â€¢ Synthesizes intelligent answer
   â€¢ Cites sources

4. REAL-WORLD TESTING
   â€¢ Answered {len(answers)} diverse questions
   â€¢ Showed varying query types
   â€¢ Demonstrated practical utility
   â€¢ Average response time: {sum(a['time'] for a in answers) / len(answers):.2f} seconds

WHY RAG WORKS:

Search alone (without LLM):
  âŒ Returns raw chunks (not useful)
  âŒ No synthesis or understanding
  âŒ Users must read all chunks themselves

LLM alone (without search):
  âŒ Can't see your data
  âŒ Would hallucinate answers
  âŒ 1,564 chunks in context = too expensive

RAG (Search + LLM):
  âœ… Fast filtering (semantic search)
  âœ… Intelligent synthesis (Claude)
  âœ… Cited sources (traceability)
  âœ… Efficient token usage
  âœ… Real-time performance

This pattern works for:
  â€¢ Customer support systems
  â€¢ Document search engines
  â€¢ Research assistants
  â€¢ Internal knowledge bases
  â€¢ Medical/legal document search
  â€¢ Q&A over company data

Your system successfully demonstrates the power of RAG!
""")

print(f"\n" + "â–ˆ"*80)
print("â–ˆ" + " "*78 + "â–ˆ")
print("â–ˆ" + " DEMO COMPLETE ".center(78) + "â–ˆ")
print("â–ˆ" + " "*78 + "â–ˆ")
print("â–ˆ"*80 + "\n")
