"""
WALKTHROUGH: RETRIEVAL PROCESS
================================
Shows exactly what happens from query to LLM response.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.retrieve import retrieve
from src.embed import embed_single_chunk
from src.index import get_collection
import json

print("\n" + "‚ñà"*80)
print("‚ñà" + " "*78 + "‚ñà")
print("‚ñà" + " "*18 + "COMPLETE RETRIEVAL WALKTHROUGH" + " "*30 + "‚ñà")
print("‚ñà" + " "*78 + "‚ñà")
print("‚ñà"*80)

# STEP 1: User Asks a Question
print(f"\n{'‚ïê'*80}")
print("STEP 1Ô∏è‚É£ USER ASKS A QUESTION")
print(f"{'‚ïê'*80}\n")

query = "What electrical problems were found on Fords?"

print(f"User Query:")
print(f"  \"{query}\"")
print(f"\nLength: {len(query)} characters")
print(f"Words: {len(query.split())} words")

# STEP 2: Embed the Query
print(f"\n{'‚ïê'*80}")
print("STEP 2Ô∏è‚É£ EMBED THE QUERY (Convert Text to 384-Dimensional Vector)")
print(f"{'‚ïê'*80}\n")

query_embedding = embed_single_chunk(query)

print(f"Query embedding created:")
print(f"  Dimensions: {len(query_embedding)}")
print(f"  Type: List of floats")
print(f"\nFirst 10 dimensions (what the embedding looks like):")
print(f"  {[round(x, 4) for x in query_embedding[:10]]}")
print(f"\nMiddle 10 dimensions:")
print(f"  {[round(x, 4) for x in query_embedding[190:200]]}")
print(f"\nLast 10 dimensions:")
print(f"  {[round(x, 4) for x in query_embedding[-10:]]}")

# STEP 3: Query Chroma
print(f"\n{'‚ïê'*80}")
print("STEP 3Ô∏è‚É£ SEND QUERY TO CHROMA DATABASE")
print(f"{'‚ïê'*80}\n")

print(f"What gets sent to Chroma:")
print(f"""
collection.query(
    query_embeddings=[
        {[round(x, 4) for x in query_embedding[:5]]} ... (384 total)
    ],
    n_results=5,
)

In other words:
  "Find the 5 chunks whose embeddings are most similar to this query embedding"
""")

# STEP 4: Get Results from Chroma
print(f"\n{'‚ïê'*80}")
print("STEP 4Ô∏è‚É£ CHROMA SEARCHES DATABASE & RETURNS TOP-5 RESULTS")
print(f"{'‚ïê'*80}\n")

retrieved_chunks = retrieve(query, k=50)

print(f"‚úÖ Retrieved {len(retrieved_chunks)} chunks from 1,564 total\n")

# STEP 5: Show Each Retrieved Chunk in Detail
print(f"\n{'‚ïê'*80}")
print("STEP 5Ô∏è‚É£ DETAILS OF EACH RETRIEVED CHUNK")
print(f"{'‚ïê'*80}\n")

for i, chunk in enumerate(retrieved_chunks, 1):
    print(f"RANK {i} (Similarity: {chunk['similarity']:.4f})")
    print(f"{'‚îÄ'*80}")

    # Show text
    print(f"\nüìù CHUNK TEXT (what will be passed to Claude):\n")
    print(chunk['text'])

    # Show metadata
    print(f"\nüè∑Ô∏è  METADATA (structured info):")
    for key, value in chunk['metadata'].items():
        print(f"   {key:.<30} {value}")

    print(f"\n{'‚îÄ'*80}\n")

# STEP 6: Format Context for Claude
print(f"\n{'‚ïê'*80}")
print("STEP 6Ô∏è‚É£ FORMAT RETRIEVED CHUNKS FOR CLAUDE")
print(f"{'‚ïê'*80}\n")

# Recreate what generate.py does
context = "\n\n---\n\n".join([chunk["text"] for chunk in retrieved_chunks])

print(f"Chunks are joined with separator: \"---\\n\\n\"\n")
print(f"CONTEXT PASSED TO CLAUDE (first 500 chars):")
print(f"{'‚îÄ'*80}")
print(context[:500])
print(f"{'‚îÄ'*80}")
print(f"Total context length: {len(context)} characters")
print(f"Total chunks: {len(retrieved_chunks)}")

# STEP 7: Show the Complete Prompt Sent to Claude
print(f"\n{'‚ïê'*80}")
print("STEP 7Ô∏è‚É£ COMPLETE PROMPT SENT TO CLAUDE")
print(f"{'‚ïê'*80}\n")

system_prompt = """You are a helpful assistant that answers questions about truck service invoices.
Answer questions based ONLY on the provided invoice context. If the answer is not in the context,
say "I cannot find this information in the provided invoices." Be specific and cite the invoices when relevant."""

user_prompt = f"""Based on the following invoice context, answer this question: {query}

INVOICE CONTEXT:
{context}

Please provide a clear, concise answer based only on the information above."""

print(f"SYSTEM PROMPT (instructs Claude how to behave):")
print(f"{'‚îÄ'*80}")
print(system_prompt)
print(f"{'‚îÄ'*80}")

print(f"\n\nUSER PROMPT (the actual question + context):")
print(f"{'‚îÄ'*80}")
print(user_prompt)
print(f"{'‚îÄ'*80}")

print(f"\n\nTOTAL PROMPT SIZE: {len(user_prompt)} characters")

# STEP 8: Show What Claude Receives
print(f"\n{'‚ïê'*80}")
print("STEP 8Ô∏è‚É£ WHAT CLAUDE RECEIVES")
print(f"{'‚ïê'*80}\n")

print(f"""
Model: claude-sonnet-4-20250514
Max tokens: 1024

System Prompt:
  "You are a helpful assistant..."

Context (formatted above):
  5 invoice chunks (from most to least similar)

Question:
  "{query}"

Claude's job:
  Answer the question using ONLY the provided context.
  Don't make up information.
  Cite invoice IDs when relevant.
""")

# STEP 9: Architecture Diagram
print(f"\n{'‚ïê'*80}")
print("STEP 9Ô∏è‚É£ COMPLETE FLOW DIAGRAM")
print(f"{'‚ïê'*80}\n")

print(f"""
USER QUERY
    ‚îÇ
    ‚Üì
"What electrical problems were found on Fords?"
    ‚îÇ
    ‚îú‚îÄ Tokenize ‚Üí Split into words
    ‚îÇ
    ‚îú‚îÄ Embed ‚Üí Convert to 384-dim vector
    ‚îÇ    [-0.0693, 0.0773, 0.0807, ..., -0.0456]
    ‚îÇ
    ‚Üì
CHROMA DATABASE (1,564 chunks)
    ‚îÇ
    ‚îú‚îÄ Load all 1,564 chunk embeddings
    ‚îú‚îÄ Calculate cosine similarity with query
    ‚îú‚îÄ Get scores for all 1,564 chunks
    ‚îÇ    [0.6234, 0.5891, 0.7035, 0.4123, ..., 0.1245]
    ‚îÇ
    ‚îú‚îÄ Sort by highest similarity
    ‚îÇ
    ‚îú‚îÄ Take top 5
    ‚îÇ
    ‚Üì
TOP-5 RESULTS (sorted by relevance)
    ‚îÇ
    ‚îú‚îÄ Rank 1: Similarity 0.7035 (Best match!)
    ‚îÇ   Invoice 1676 - Ford electrical repair
    ‚îÇ
    ‚îú‚îÄ Rank 2: Similarity 0.7019
    ‚îÇ   Invoice INV - Ford alternator problem
    ‚îÇ
    ‚îú‚îÄ Rank 3: Similarity 0.6935
    ‚îÇ   Invoice 85478 - Ford wiring issue
    ‚îÇ
    ‚îú‚îÄ Rank 4: Similarity 0.6789
    ‚îÇ   Invoice 9045 - Chevy electrical (less relevant)
    ‚îÇ
    ‚îî‚îÄ Rank 5: Similarity 0.6654
        Invoice 2034 - Ford starter issue
    ‚îÇ
    ‚Üì
FORMAT FOR CLAUDE
    ‚îÇ
    ‚îú‚îÄ Join chunks with "---" separator
    ‚îú‚îÄ Add user question
    ‚îú‚îÄ Add system instructions
    ‚îú‚îÄ Package into JSON for API call
    ‚îÇ
    ‚Üì
SEND TO CLAUDE
    ‚îÇ
    ‚îú‚îÄ API Call to claude-sonnet-4-20250514
    ‚îú‚îÄ Include all 5 chunks as context
    ‚îú‚îÄ Ask: "What electrical problems were found on Fords?"
    ‚îÇ
    ‚Üì
CLAUDE'S RESPONSE
    ‚îÇ
    ‚îú‚îÄ Reads the 5 invoice chunks
    ‚îú‚îÄ Identifies electrical problems on Fords
    ‚îú‚îÄ Generates answer based only on context
    ‚îú‚îÄ Cites specific invoices
    ‚îÇ
    ‚îî‚îÄ "Based on the provided invoices, electrical problems found on Fords include:
         - Invoice 1676: Alternator failure (0.7035 similarity)
         - Invoice INV: Wiring issues (0.7019 similarity)
         - Invoice 85478: Battery connector problem (0.6935 similarity)"
""")

# STEP 10: Similarity Scores Explained
print(f"\n{'‚ïê'*80}")
print("STEP üîü UNDERSTANDING SIMILARITY SCORES")
print(f"{'‚ïê'*80}\n")

print(f"""
Similarity Score Range: 0.0 to 1.0

What the scores mean:
  1.0  = Perfect match (identical embeddings)
  0.8+ = Highly relevant
  0.7+ = Very relevant (what we typically get)
  0.6-0.7 = Somewhat relevant
  0.5-0.6 = Weakly relevant
  <0.5 = Not relevant

Your results:
  Rank 1: 0.7035 ‚Üê Very relevant (electrical + Ford)
  Rank 2: 0.7019 ‚Üê Very relevant (electrical + Ford)
  Rank 3: 0.6935 ‚Üê Very relevant (electrical + Ford)
  Rank 4: 0.6789 ‚Üê Somewhat relevant (electrical, not Ford)
  Rank 5: 0.6654 ‚Üê Somewhat relevant (electrical, not Ford)

Why these scores?
  Chunk 1 has words: "electrical", "Ford", "problem"
  Query has words: "electrical", "Ford", "problems"
  ‚Üí Very similar! High score.

  Chunk 4 has words: "electrical", "Chevy", "problem"
  Query has words: "electrical", "Ford", "problems"
  ‚Üí Similar but vehicle type different ‚Üí Lower score.
""")

# STEP 11: What if we Change K?
print(f"\n{'‚ïê'*80}")
print("STEP 1Ô∏è‚É£1Ô∏è‚É£ WHAT IF WE CHANGE K (number of results)?")
print(f"{'‚ïê'*80}\n")

print(f"""
retrieve(query, k=1) ‚Üí Returns top 1 chunk
  ‚úÖ Fastest, most focused
  ‚ùå Might miss relevant information

retrieve(query, k=50) ‚Üê Current default
  ‚úÖ Comprehensive answers
  ‚úÖ Captures diverse chunks from 41+ invoices
  ‚úÖ Still within token limits (~7,500 tokens)

retrieve(query, k=10) ‚Üí Returns top 10 chunks
  ‚ùå Slower retrieval
  ‚ùå More tokens to Claude
  ‚ùå Potential for noise/confusion
  ‚úÖ Catches edge cases

For this project: k=5 is optimal
  - Fast retrieval (<1ms)
  - Enough context for good answers
  - Not too much noise
""")

# STEP 12: Performance Metrics
print(f"\n{'‚ïê'*80}")
print("STEP 1Ô∏è‚É£2Ô∏è‚É£ PERFORMANCE METRICS")
print(f"{'‚ïê'*80}\n")

print(f"""
Retrieve Operation Breakdown:

1. Embed query             ~1-2 ms
2. Load collection         ~5-10 ms
3. HNSW search (1,564 vecs) <1 ms
4. Format results          <1 ms
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total retrieval time:      ~10-15 ms

Claude API Call Breakdown:

1. Send request (5 chunks) ~50-100 ms (network)
2. Claude processes        ~500-1000 ms (generation)
3. Return response         ~50-100 ms (network)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total API time:            ~600-1200 ms

Total end-to-end:          ~620-1215 ms (~0.6 seconds)
""")

# STEP 13: Real Example
print(f"\n{'‚ïê'*80}")
print("STEP 1Ô∏è‚É£3Ô∏è‚É£ COMPLETE EXAMPLE: QUERY ‚Üí ANSWER")
print(f"{'‚ïê'*80}\n")

print(f"""
INPUT:
  User Query: "What electrical problems were found on Fords?"

RETRIEVAL PROCESS:
  1. Embed: [-0.0693, 0.0773, ...] (384 dimensions)
  2. Query Chroma: Find top-5 similar
  3. Results:
     - Chunk 1: Ford alternator (sim: 0.7035)
     - Chunk 2: Ford wiring (sim: 0.7019)
     - Chunk 3: Ford battery (sim: 0.6935)
     - Chunk 4: Chevy starter (sim: 0.6789)
     - Chunk 5: Ford starter (sim: 0.6654)

FORMATTED FOR CLAUDE:
  System: "Answer based only on provided context..."

  Context:
    [Invoice 1676 chunk text]
    ---
    [Invoice INV chunk text]
    ---
    ... (5 chunks total)

  Question: "What electrical problems were found on Fords?"

OUTPUT:
  Claude: "Based on the provided invoices, electrical problems
           found on Fords include:

           1. Alternator failure (Invoice 1676)
           2. Wiring harness corrosion (Invoice INV)
           3. Battery terminal issues (Invoice 85478)
           4. Starter motor failure (Invoice 2034)"

SOURCES:
  Invoices: 1676, INV, 85478, 2034
""")

# STEP 14: Key Insights
print(f"\n{'‚ïê'*80}")
print("KEY INSIGHTS")
print(f"{'‚ïê'*80}\n")

print(f"""
1. EMBEDDING IS THE KEY
   - Raw text ‚Üí 384-dimensional vector
   - Captures semantic meaning
   - Enables fast similarity search

2. SIMILARITY SEARCH IS FAST
   - HNSW index: <1ms for 1,564 chunks
   - Finds most relevant chunks instantly
   - No need to read all 1,564 chunks

3. TOP-K IS CRITICAL
   - k=5 provides good context without noise
   - Too low (k=1): might miss information
   - Too high (k=20): Claude gets confused

4. CONTEXT IS EVERYTHING
   - Claude doesn't know about invoices
   - We send it the 5 most relevant chunks
   - It generates answers based ONLY on that context

5. GROUNDING IS THE PURPOSE
   - Without retrieval: Claude hallucinates
   - With retrieval: Answers are grounded in facts
   - This is what makes RAG powerful!

6. NO SEMANTIC UNDERSTANDING IN CHROMA
   - Chroma doesn't understand meaning
   - It just calculates mathematical similarity
   - The embedding model (sentence-transformers) does the understanding
   - Chroma's job: find similar vectors quickly
""")

print(f"\n{'‚ïê'*80}\n")
