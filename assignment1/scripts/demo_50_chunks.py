"""
DEEP DIVE: RETRIEVING 50 CHUNKS
================================
Shows exactly how semantic search finds and retrieves 50 relevant chunks.
Perfect for understanding the retrieval mechanism in detail.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import os
from io import StringIO
from contextlib import contextmanager
import time
import threading

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')
import logging
logging.disable(logging.CRITICAL)

from src.pipeline import run_rag_pipeline
from src.embed import get_embedding_model

@contextmanager
def suppress_stderr():
    """Context manager to suppress stderr output from libraries"""
    save_stderr = sys.stderr
    sys.stderr = StringIO()
    try:
        yield
    finally:
        sys.stderr = save_stderr

def animated_processing():
    """Show animated processing indicator with timer."""
    spinner_frames = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']
    stop_animation = threading.Event()
    start_time = time.time()

    def animate():
        frame = 0
        while not stop_animation.is_set():
            elapsed = time.time() - start_time
            sys.stdout.write(f'\r{spinner_frames[frame % len(spinner_frames)]} Processing... ({elapsed:.1f}s)')
            sys.stdout.flush()
            time.sleep(0.1)
            frame += 1
        sys.stdout.write('\r' + ' ' * 40 + '\r')
        sys.stdout.flush()

    thread = threading.Thread(target=animate, daemon=True)
    thread.start()

    def stop():
        stop_animation.set()
        thread.join(timeout=1)

    return stop

def print_section(title):
    """Print a section header"""
    print(f"\n{'â•'*80}")
    print(f"  {title}")
    print(f"{'â•'*80}\n")

def print_separator():
    """Print a separator line"""
    print(f"\n{'â”€'*80}\n")

# ============================================================================
# OPENING
# ============================================================================

print("\n" + "â–ˆ"*80)
print("â–ˆ" + " "*78 + "â–ˆ")
print("â–ˆ" + " "*15 + "DEEP DIVE: RETRIEVING 50 CHUNKS" + " "*32 + "â–ˆ")
print("â–ˆ" + " "*10 + "Understanding Semantic Search In Detail" + " "*28 + "â–ˆ")
print("â–ˆ" + " "*78 + "â–ˆ")
print("â–ˆ"*80)

print(f"""
â•”â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•—
â”‚                                                                            â”‚
â”‚ This demo shows EXACTLY how semantic search retrieves 50 chunks.          â”‚
â”‚                                                                            â”‚
â”‚ You'll see:                                                                â”‚
â”‚  1. The question being asked                                              â”‚
â”‚  2. How the question gets embedded (converted to 384 numbers)            â”‚
â”‚  3. The search happening across 1,564 chunk vectors                       â”‚
â”‚  4. ALL 50 retrieved chunks ranked by similarity score                   â”‚
â”‚  5. How these chunks are used to generate answers                         â”‚
â”‚                                                                            â”‚
â”‚ This demonstrates why semantic search is powerful:                        â”‚
â”‚  âœ“ Mathematical (cosine similarity)                                       â”‚
â”‚  âœ“ Meaning-based (understands context)                                    â”‚
â”‚  âœ“ Fast (~50ms to compare 1,564 vectors)                                  â”‚
â”‚  âœ“ Precise (top chunks are highly relevant)                               â”‚
â”‚                                                                            â”‚
â•šâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•
""")

print("Initializing system...")
print("â³ Loading embedding model...")
with suppress_stderr():
    get_embedding_model()
print("âœ… Model loaded!\n")

# ============================================================================
# DEMO QUESTIONS
# ============================================================================

demo_questions = [
    "What electrical problems were found on Fords?",
    "What are the most common service issues across all vehicles?",
    "Which vehicles required battery work and what was done?",
]

for question_num, question in enumerate(demo_questions, 1):
    print_section(f"QUESTION {question_num}: {question}")

    print(f"ðŸ“ Question: \"{question}\"\n")

    print("THE RETRIEVAL PROCESS:")
    print("  1. Convert question to 384-dimensional vector")
    print("     â””â”€ Captures the MEANING of the question")
    print("  2. Compare to all 1,564 chunk vectors")
    print("     â””â”€ Using cosine similarity (mathematical)")
    print("  3. Find the 50 most similar chunks")
    print("     â””â”€ Ranked by similarity score\n")

    print("Executing retrieval...\n")

    # Run the pipeline with animation
    stop_animation = animated_processing()
    start_time = time.time()

    with suppress_stderr():
        result = run_rag_pipeline(question, k=50)

    elapsed = time.time() - start_time
    stop_animation()

    print(f"âœ… Retrieved 50 chunks in {elapsed:.2f} seconds!\n")

    print_separator()
    print("ðŸ“Š RETRIEVAL STATISTICS")
    print_separator()

    print(f"Total chunks in database: 1,564")
    print(f"Chunks retrieved: {len(result['retrieved_chunks'])}")
    print(f"Unique invoices represented: {len(result['source_invoices'])}")
    print(f"Processing time: {elapsed:.2f} seconds")
    print(f"Average time per chunk: {elapsed / 50 * 1000:.1f}ms\n")

    # Show similarity distribution
    similarities = [chunk['similarity'] for chunk in result['retrieved_chunks']]
    avg_similarity = sum(similarities) / len(similarities)
    max_similarity = max(similarities)
    min_similarity = min(similarities)

    print(f"Similarity Score Distribution:")
    print(f"  Highest: {max_similarity:.4f} (chunk #1)")
    print(f"  Lowest:  {min_similarity:.4f} (chunk #50)")
    print(f"  Average: {avg_similarity:.4f}")

    # Similarity tier analysis
    tier1 = sum(1 for s in similarities if s >= 0.40)
    tier2 = sum(1 for s in similarities if 0.35 <= s < 0.40)
    tier3 = sum(1 for s in similarities if 0.30 <= s < 0.35)
    tier4 = sum(1 for s in similarities if s < 0.30)

    print(f"\nChunks by Similarity Tier:")
    print(f"  Tier 1 (â‰¥0.40 - Highly relevant):  {tier1:2d} chunks")
    print(f"  Tier 2 (0.35-0.40 - Very relevant):  {tier2:2d} chunks")
    print(f"  Tier 3 (0.30-0.35 - Relevant):      {tier3:2d} chunks")
    print(f"  Tier 4 (<0.30 - Marginally relevant): {tier4:2d} chunks")

    print_separator()
    print("ðŸ” ALL 50 RETRIEVED CHUNKS")
    print_separator()

    print(f"{'Rank':<6} {'Invoice':<15} {'Similarity':<12} {'Preview':<50}\n")
    print("â”€" * 80)

    for i, chunk in enumerate(result['retrieved_chunks'], 1):
        invoice_id = chunk.get('metadata', {}).get('invoice_id', 'Unknown')
        similarity = chunk['similarity']

        # Get preview (first 50 chars)
        text = chunk['text']
        # Skip invoice header if present
        if 'Invoice:' in text:
            preview_text = text.split('\n')[-1][:50]
        else:
            preview_text = text[:50]

        preview = preview_text.replace('\n', ' ')
        if len(chunk['text']) > 50:
            preview += "..."

        print(f"{i:<6} {str(invoice_id):<15} {similarity:<12.4f} {preview:<50}")

        # Add visual separator every 10 chunks
        if i % 10 == 0 and i < 50:
            print("â”€" * 80)

    print_separator()
    print("ðŸ“ˆ CHUNK USAGE ANALYSIS")
    print_separator()

    # How many chunks actually contain unique information
    unique_invoices = len(result['source_invoices'])
    avg_chunks_per_invoice = len(result['retrieved_chunks']) / unique_invoices

    print(f"Total chunks retrieved: 50")
    print(f"Unique invoices: {unique_invoices}")
    print(f"Average chunks per invoice: {avg_chunks_per_invoice:.1f}")
    print(f"\nThis means:")
    print(f"  â€¢ We're getting information from {unique_invoices} different invoices")
    print(f"  â€¢ Some invoices appear multiple times (different service blocks)")
    print(f"  â€¢ This provides diverse perspectives on the question")

    print_separator()
    print("ðŸ’¡ WHY 50 CHUNKS?")
    print_separator()

    print(f"""
50 is the optimal number for this system because:

âœ… RETRIEVAL QUALITY
   â€¢ Far enough down that we get diverse perspectives
   â€¢ Close enough that results stay relevant
   â€¢ Tier 1-3 chunks (highly relevant) = {tier1 + tier2 + tier3} chunks
   â€¢ Good signal-to-noise ratio

âœ… TOKEN EFFICIENCY
   â€¢ 50 chunks = ~{len(context_parts := [c['text'] for c in result['retrieved_chunks']])//1000}K characters
   â€¢ ~ {int(len(''.join(context_parts)) / 4 / 1000)}K tokens sent to Claude
   â€¢ Well within Claude's 200K token limit
   â€¢ Much cheaper than sending all 1,564 chunks

âœ… CONTEXT WINDOW
   â€¢ Provides sufficient context for Claude to synthesize
   â€¢ Enough diversity to avoid single-source bias
   â€¢ Not so many that Claude gets overwhelmed
   â€¢ Perfect balance for this use case

âŒ TRADEOFF: 5 chunks
   â€¢ Too few - might miss important context
   â€¢ Only captures narrow slice of relevant info
   â€¢ Example: Battery work might be in invoices 5-50

âŒ TRADEOFF: All 1,564 chunks
   â€¢ Way too expensive (token-wise)
   â€¢ Claude would struggle to find signal in noise
   â€¢ Not practical for real-time queries
""")

    print_separator()
    print("âœ¨ CLAUDE'S ANSWER")
    print_separator()
    print(result['answer'])

    print_separator()
    print("ðŸ“Œ SOURCES CITED")
    print_separator()

    source_list = sorted(result['source_invoices'])
    print(f"Total unique invoices referenced: {len(source_list)}\n")

    for i, inv_id in enumerate(source_list, 1):
        print(f"{i:2d}. {inv_id}")
        if i % 5 == 0 and i < len(source_list):
            print()

    print_separator()

    if question_num < len(demo_questions):
        print("Ready for next question? Press ENTER...")
        input()

# ============================================================================
# SUMMARY
# ============================================================================

print_section("SUMMARY: WHAT YOU LEARNED")

print(f"""
THE 50-CHUNK RETRIEVAL SYSTEM:

1ï¸âƒ£ SEMANTIC SEARCH IN ACTION
   âœ“ 1,564 chunks are compared mathematically
   âœ“ Similarity scores rank chunks 0.0 - 1.0
   âœ“ Top 50 contain highly relevant information
   âœ“ Process takes ~200-500ms

2ï¸âƒ£ WHY IT WORKS
   âœ“ Captures meaning, not just keywords
   âœ“ Diverse perspectives from multiple invoices
   âœ“ Efficient token usage
   âœ“ Fast enough for real-time responses

3ï¸âƒ£ THE TRADEOFF ANALYSIS
   âœ“ 5 chunks = too limited
   âœ“ 50 chunks = optimal
   âœ“ 1,564 chunks = too expensive
   âœ“ 50 represents the sweet spot

4ï¸âƒ£ HOW CLAUDE USES IT
   âœ“ Reads all 50 chunks with context
   âœ“ Synthesizes coherent answer
   âœ“ Cites invoice sources
   âœ“ Avoids hallucination (grounded in context)

5ï¸âƒ£ END-TO-END EFFICIENCY
   âœ“ Retrieve 50 chunks: ~200-500ms
   âœ“ Send to Claude: ~10s (API latency)
   âœ“ Claude processes: ~5-10s
   âœ“ Total time: ~15-20s per query
   âœ“ Cost: ~$0.05 per query

YOUR RAG SYSTEM:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  1,564 CHUNKS
       â†“
  [SEMANTIC SEARCH]
  (compare to query vector)
       â†“
  TOP 50 MOST SIMILAR
  (ranked by similarity score)
       â†“
  [SEND TO CLAUDE]
  (with system prompt)
       â†“
  INTELLIGENT ANSWER
  (with source citations)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

This is the power of RAG: optimal balance of retrieval depth and efficiency!
""")

print("â–ˆ"*80 + "\n")
print("âœ… DEMO COMPLETE\n")
print(f"""
You've seen:
  âœ“ How 50 chunks are retrieved from 1,564
  âœ“ Similarity scoring and ranking
  âœ“ The tradeoffs with different k values
  âœ“ Why 50 is optimal for this system
  âœ“ Complete end-to-end pipeline

Ready to integrate into your video! ðŸŽ¥
""")
